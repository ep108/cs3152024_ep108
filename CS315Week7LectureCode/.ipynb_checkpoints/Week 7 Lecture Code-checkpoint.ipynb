{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c975a0c6-bdfb-4756-b87e-ca68a05f5336",
   "metadata": {},
   "source": [
    "# Week 7 Lecture Code: Word Embeddings / Clustering / tSNE visualization\n",
    "\n",
    "**March 2024**\n",
    "\n",
    "This is code that was discussed in the class notes for the two lectures in Week 7.\n",
    "\n",
    "**Table of Content**\n",
    "\n",
    "1. [Installing packages](#sec1)\n",
    "2. [Generating Word Embeddings and heatmaps for cosine similarity](#sec2)\n",
    "3. [K-means clustering with word embeddings](#sec3)\n",
    "4. [Using the Elbow method to find the best k for k-means](#sec4)\n",
    "5. [t-SNE visualization](#sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15700c2-2db5-4123-adbf-87755d31dd4c",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "## 1. Installing Packages\n",
    "\n",
    "This notebook contains several packages that we are using for the first time. Additionally, we might need to use these packages for the project too, thus, it's good if we install them within the virtual environment of the project.  \n",
    "\n",
    "However, if we install the packages within Jupyter, they are installed in our system-wide Python installation. We need to take some extra steps to ensure that the new packages are installed for our project-specific virtual environment. Here is what you need to do:\n",
    "\n",
    "1. Make sure that you have created the virtual environment for Project 2 (about collecting data with PykTok). You can find the instructions under Week 7 Programming Activities.\n",
    "2. Open a terminal window, navigate to the folder where you created your virtual environment, and then type `source .project2/bin/activate`.\n",
    "3. Install the following package: `pip install ipykernel`, to manage virtual environments within Jupyter notebooks.\n",
    "4. Register our virtual environment with Jupyter: `python -m ipykernel install --user --name=.project2 --display-name=\"CS315 Project2`\n",
    "5. At this point close this notebook and then restart it again.\n",
    "6. When you reopen the notebook, find the button \"Python 3 (ipykernel)\" on the top right corner. Click on it, you should see the following dialog, where you should select our virtual environment that we gave the display name CS315 Project2: ![Screenshot: Kernel selection](https://cs.wellesley.edu/~eni/cs315/choose-kernel.png)\n",
    "7. Before moving on, run the cell below, just to make sure that you are really running the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b598aa2-6f7b-4bda-a206-92738cb72868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/edithpo/Downloads/CS315_Project2_Group1/.project2/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb68a5c-ab89-4854-b330-07b3566fcf4a",
   "metadata": {},
   "source": [
    "If all the prior steps were successful, you should be able to see something similar to the path below, including `.project2/bin/python`\n",
    "![Screenshot: Path](https://cs.wellesley.edu/~eni/cs315/path-confirmation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6bfb92-281f-489e-93c4-10412413f213",
   "metadata": {},
   "source": [
    "Now you are ready to continue with the following installations for libraries used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5f3054-6cc6-4f72-94d6-6b83014b1069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp310-cp310-macosx_10_15_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tensorflow_hub\n",
      "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting plotly\n",
      "  Using cached plotly-5.20.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting nbformat\n",
      "  Downloading nbformat-5.10.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Using cached h5py-3.10.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from tensorflow) (69.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.62.1-cp310-cp310-macosx_10_10_x86_64.whl\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-macosx_10_14_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow_hub)\n",
      "  Using cached tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from seaborn) (2.2.1)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-macosx_10_12_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting tenacity>=6.2.0 (from plotly)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastjsonschema (from nbformat)\n",
      "  Downloading fastjsonschema-2.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting jsonschema>=2.6 (from nbformat)\n",
      "  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: jupyter-core in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from nbformat) (5.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (23.2.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat)\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat)\n",
      "  Downloading referencing-0.34.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat)\n",
      "  Downloading rpds_py-0.18.0-cp310-cp310-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: rich in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.0)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Using cached namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Downloading optree-0.11.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading fonttools-4.50.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading pillow-10.3.0-cp310-cp310-macosx_10_10_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from jupyter-core->nbformat) (4.2.0)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/edithpo/Downloads/CS315_Project2_Group1/.project2/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.16.1-cp310-cp310-macosx_10_15_x86_64.whl (259.5 MB)\n",
      "Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading scikit_learn-1.4.1.post1-cp310-cp310-macosx_10_9_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached plotly-5.20.0-py3-none-any.whl (15.7 MB)\n",
      "Downloading nbformat-5.10.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.10.0-cp310-cp310-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.3-cp310-cp310-macosx_10_12_x86_64.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached ml_dtypes-0.3.2-cp310-cp310-macosx_10_9_universal2.whl (389 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Downloading scipy-1.13.0-cp310-cp310-macosx_10_9_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-macosx_10_14_x86_64.whl (2.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "Downloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached wrapt-1.16.0-cp310-cp310-macosx_10_9_x86_64.whl (37 kB)\n",
      "Downloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n",
      "Downloading contourpy-1.2.1-cp310-cp310-macosx_10_9_x86_64.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.8/260.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.50.0-cp310-cp310-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Downloading kiwisolver-1.4.5-cp310-cp310-macosx_10_9_x86_64.whl (68 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp310-cp310-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading referencing-0.34.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.18.0-cp310-cp310-macosx_10_12_x86_64.whl (335 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "Downloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp310-cp310-macosx_10_9_x86_64.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.1/295.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_x86_64.whl (14 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, fastjsonschema, wrapt, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, tenacity, scipy, rpds-py, pyparsing, protobuf, pillow, optree, opt-einsum, ml-dtypes, MarkupSafe, markdown, kiwisolver, joblib, h5py, grpcio, google-pasta, gast, fonttools, cycler, contourpy, astunparse, absl-py, werkzeug, scikit-learn, referencing, plotly, matplotlib, tensorboard, seaborn, keras, jsonschema-specifications, tensorflow, jsonschema, tf-keras, nbformat, tensorflow_hub\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow tensorflow_hub scikit-learn seaborn plotly nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57827f06-5769-4441-8d93-73118488f8a8",
   "metadata": {},
   "source": [
    "If the installation was successful, you are ready to continue. If you run into issues, let me know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd28d25-de89-4f58-9c6b-dd47061d95e9",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## 2. Generating word embeddings\n",
    "\n",
    "We will use the `tensorflow` library that loads the Universal Sentence Encoder from Google. Running the cells below takes several seconds, this is why it's good to have a notebook that you keep open and running to avoid having to repeat these steps everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c1b7aa-73e2-4b3c-b9a8-b04f75c89489",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd02bf8-f384-40e8-83dc-a0ed68975ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317bba7-bb46-4214-b3f2-e2574131ea88",
   "metadata": {},
   "source": [
    "Let's see what `embed` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77205152-c887-43b7-a717-10dd72f27d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655b878-13fc-47be-ad5f-d585b7649d81",
   "metadata": {},
   "source": [
    "It seems like instance of a class that is loading a saved model for the user.\n",
    "\n",
    "We know that `embed` expects a list of words, thus, even if we want the embedding of one single word, we put it in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33063607-2fe3-4193-84f1-0011ab8ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the word embedding of a single word\n",
    "embed([\"apple\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761e9eb-7a8b-448e-ad6e-c4c3bbcb88e4",
   "metadata": {},
   "source": [
    "From the result, we can see that we got a tensor of shape 1x512, meaning a single row with 512 columns (the dimensions for the word representation). A tensor is a mathematical term for a multi-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d916a-434f-4387-9aa0-105c731a3876",
   "metadata": {},
   "source": [
    "Let's get the embeddings for a few pairs of words that we know are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e24667-0b6c-4df3-b84a-5303947c7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['king', 'queen', 'radio', 'TV', 'bike', 'car', 'Boston', 'London', 'lake', 'river']\n",
    "\n",
    "embeddings = embed(words)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049cf657-ebcb-4eb4-b9c6-23bcd27f0b5b",
   "metadata": {},
   "source": [
    "As we can see, we got one embedding for each word as a vector of 512 dimensions. We will calculate the cosine similarity between these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc9362-9d1b-4e8e-9b5b-38b072166286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosineSimilarity(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    V1 = np.array(vec1)\n",
    "    V2 = np.array(vec2)\n",
    "    cosine = np.dot(V1, V2)/(norm(V1)*norm(V2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cd67d-1457-4394-a8ec-8e40fe2bd00a",
   "metadata": {},
   "source": [
    "Now that we have this function, we will call it to calculate the similarity between all pairs of words, as part of a new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8337354-1f4d-4234-bf1c-574d4d3a6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwiseSimilarity(embeddings):\n",
    "    \"\"\"Given a matrix of embeddings for words or sentences,\n",
    "    calculate the cosine similarity for each pair.\n",
    "    \"\"\"\n",
    "    simMatrix = []\n",
    "    for vec1 in embeddings:\n",
    "        simRow = []\n",
    "        for vec2 in embeddings:\n",
    "            simRow.append(cosineSimilarity(vec1, vec2))\n",
    "        simMatrix.append(simRow)\n",
    "    return simMatrix\n",
    "    \n",
    "simMatrix = pairwiseSimilarity(embeddings)\n",
    "print(simMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b529cb-4be6-48de-be1f-aa8deca6e0a8",
   "metadata": {},
   "source": [
    "Let's generate a heatmap to see the similarity between the pairs of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413dcbc-5994-4edd-a35a-180536a2cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def drawHeatmap(labels, simMtrx, plotTitle):\n",
    "    \"\"\"Draws a heatmap for the similarity matrix.\n",
    "    \"\"\"\n",
    "    sns.set(font_scale=0.9)\n",
    "    g = sns.heatmap(\n",
    "          simMtrx, # similarity matrix with the cosine sim values\n",
    "          xticklabels=labels,\n",
    "          yticklabels=labels,\n",
    "          vmin=0,\n",
    "          vmax=1,\n",
    "          cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=90)\n",
    "    g.set_title(plotTitle, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8bf97a-ef82-468e-8c42-2d984a24b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawHeatmap(words, simMatrix, \"Similarity for Word Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6d950-ba9f-47a8-8225-87e1d49ecbe1",
   "metadata": {},
   "source": [
    "**Optional Challenge: Find most similar words**\n",
    "\n",
    "While the heat map gives us some information, is hard to see the most simlar words. We can write a Python function to find the most similar pairs. I wrote one that I can call like this:\n",
    "\n",
    "`findUniqueTopPairs(simMatrix, words)[:10]`\n",
    "\n",
    "And the results are the following:\n",
    "```\n",
    "[(0.58856946, 'king', 'queen'),\n",
    " (0.56984055, 'bike', 'car'),\n",
    " (0.49752563, 'Boston', 'London'),\n",
    " (0.471712, 'lake', 'river'),\n",
    " (0.45328128, 'TV', 'radio'),\n",
    " (0.45171037, 'car', 'radio'),\n",
    " (0.43531647, 'TV', 'car'),\n",
    " (0.3785724, 'bike', 'river'),\n",
    " (0.3519483, 'London', 'king'),\n",
    " (0.35168734, 'car', 'river')]\n",
    "```\n",
    "\n",
    "Notice how our original 5 pairs are at the top, and pairs like 'car' and 'radio' are still related. One can also understand the relationshp between 'London' and 'king' (based on the history of the many kings who lived in London, although for the past 70 years there was a queen in the palace). Anyway, we can see how the similarity is going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086377d4-d2f8-42ee-b272-31e563385714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e9813-8631-499a-908a-7dcc42d9eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aab7697-d611-4795-88eb-eeadd4f0294f",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>\n",
    "## 3. K-means clustering with word embeddings\n",
    "\n",
    "In lecture we showed the clustering for the TikTok hashtags collected from posts, but for connecting this section to the t-SNE visualization section below, we will show here the clustering for our own list of news hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67740a-4061-4100-a7dd-872b7a23fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the news hashtags\n",
    "import json\n",
    "news = json.load(open('news-hashtags.json'))\n",
    "news[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e8d39-318a-4c8d-aaf8-efaaa479c28d",
   "metadata": {},
   "source": [
    "**Step 1: Get the word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27a2d6-5999-4fe3-9efe-3eb3bdccf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that we loaded the model in the prior section\n",
    "newsEmbed = embed(news)\n",
    "newsEmbed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da04f34-32e6-4cb6-9861-f5deec14d99b",
   "metadata": {},
   "source": [
    "**Step 2: Perform clustering with a fixed k value**\n",
    "\n",
    "For this time, we will perform clustering with k=15. The code will take a few seconds (or more) to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d57f6-be4d-49f1-a224-e15de04cc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 15 # number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(newsEmbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737b1ba-2193-489b-9c49-d5b2e99f020a",
   "metadata": {},
   "source": [
    "Let's see what the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f413849-ddef-46f8-8394-a7577a86d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73337644-2a34-4901-81bf-dd2e27d6aca1",
   "metadata": {},
   "source": [
    "They are indices of the clusters. For each elemenent in our `news` list, the `clusters` array indicates in which of the 15 clusters the item has been assigned. We can then use this information to find out which words are in which cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a0fad-6fd1-4744-bd28-63f6020f2737",
   "metadata": {},
   "source": [
    "We will print out the composition of each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d3343-9151-46e5-95d3-c1e081279cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    cluster_words = [news[j] for j in range(len(news)) if clusters[j] == i]\n",
    "    print(cluster_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f283a3-bb5c-4cc9-8fdf-6cce66fc2fc4",
   "metadata": {},
   "source": [
    "As we discussed when we looked at the t-SNE visualization in class, some groups of words are really meaningful, for example, Cluster 5, Cluster 6, Cluster 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d0461-69f6-42e1-bc3d-012392953006",
   "metadata": {},
   "source": [
    "<a id=\"sec4\"></a>\n",
    "## 4. Using the Elbow method to find the best k\n",
    "\n",
    "We discussed that having to pick k is a limitation of K-means clustering. Below, we show one method known as the Elbow method to try to pick the best k. If you want to learn about the method, here is [a short 4-minute video](https://www.youtube.com/watch?v=ht7geyMAFfA) to explain it. It is called the Elbow method because k is selected at the lowest point that marks something that looks like an elbow. This is the point where the sum of squared distances from the cluster centroid stops decreasing rapidly and enters a phase of a constant-paced decline, toward 0. This sum of squared distances of all points from the cluster centroid is known as inertia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73271b-7338-4940-90eb-8cbd93323dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbowMethod(embeddings, maxK):\n",
    "    \"\"\"\n",
    "    Implements the Elbow method for finding most optimal k.\n",
    "    It keeps track of a measure named \"inertia\" for each cluster.\n",
    "    \"\"\"\n",
    "    sumSquaredDistances = []\n",
    "    kValues = list(range(1, maxK))\n",
    "    for k in kValues:\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        km = km.fit(embeddings)\n",
    "        sumSquaredDistances.append(km.inertia_)\n",
    "    \n",
    "    # plot the line to identify the elbow\n",
    "    plt.plot(kValues, sumSquaredDistances, 'ro-')\n",
    "    plt.xlabel('k')\n",
    "    plt.xticks(kValues)\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75975e-3c67-4bff-8d51-bc6e49c01ac3",
   "metadata": {},
   "source": [
    "Let's try below 20 values for k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e8fcc-d4db-41e2-bd3f-4da0e0d04b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbowMethod(newsEmbed, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3ffeb-336c-40eb-9004-a9746595830f",
   "metadata": {},
   "source": [
    "In this plot, inertia stops rapidly decreasing at the k=3 and enters a phase of steady decline. So we will pick k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b738d-e873-4f5e-8bf7-5b22df792c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(newsEmbed)\n",
    "\n",
    "clusters[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4416b3-880e-4c60-8c54-5264078866ea",
   "metadata": {},
   "source": [
    "Notices the indices of the clusters: 0, 1, 2. We can check quickly how many items are in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc1ac9-c2a0-4bea-9ef9-52856510e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84f73d-0805-4706-aedc-21e1099b9b8d",
   "metadata": {},
   "source": [
    "So, it looks like the clusters have roughly the same size: 0 and 2 have about 40 items and cluster 1 has 50 items. Let's visualize the clusters using t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6402d-25e9-4959-9db9-3c025fd7dc15",
   "metadata": {},
   "source": [
    "<a id=\"sec5\"></a>\n",
    "## 5. t-SNE visualization\n",
    "\n",
    "To create the t-SNE visualization, we need a few steps to prepare the data first. We work directly with the embeddings and then use the cluster membership as a column that will allow us to color the dots in the scatterplot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a5c21-8759-4fc9-8072-b2634209abfa",
   "metadata": {},
   "source": [
    "**Step 1: Run the TSNE algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab1c9d-abcb-4a25-9eef-7d07f28fbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)  \n",
    "tsne_results = tsne.fit_transform(newsEmbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7526c-d9b6-4b26-9cd7-bde75d5b4247",
   "metadata": {},
   "source": [
    "**Step 2: Create a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028c41c-bd4f-474b-9e6b-d2201e3e72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tsne_results, columns=['tsne_1', 'tsne_2'])\n",
    "df['hashtag'] = news  \n",
    "df['cluster'] = clusters # the cluster indices where each news hashtags belong\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39a0cb-f63f-4093-80b7-fe84ca52945e",
   "metadata": {},
   "source": [
    "**Step 3: Generate Plotly visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca5f2b-543a-4227-a8ee-b1969ef63f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(df, x='tsne_1', y='tsne_2', text='hashtag', color=\"cluster\", color_continuous_scale=\"BlueRed\")\n",
    "\n",
    "# Format what to show next to the markers\n",
    "fig.update_traces(textposition='top center', \n",
    "                  mode='markers+text', \n",
    "                  textfont=dict(size=6))\n",
    "\n",
    "fig.update_layout(title='Embeddings of TikTok News Hashtags', width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925f8c2-96bc-48bf-91b3-c200ea63250b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS315 Project2",
   "language": "python",
   "name": ".project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
